import math

import torch
import torch.nn.functional as F
from torch import nn
from torch.nn import TransformerEncoder

from layers import MFCC, Attention, ConvBlock, ConvNorm, LinearNorm


def build_model(model_params={}, model_type='asr'):
    model = ASRCNN(**model_params)
    return model


class ASRCNN(nn.Module):
    """
    The model's output depends on how it is called:                                                                                                                                                                                                                       

    1 Without Text Input:                                                                                                                                                                                                                                                
    When the model's forward method is invoked with only audio input (mel spectrogram), it outputs a tensor (ctc_logit) that contains the raw token logits. These logits are intended for Connectionist Temporal Classification (CTC) loss computations, which are used
    in many ASR systems.   

    2 With Text Input:                                                                                                                                                                                                                                                   
    When text input is provided (in addition to the audio), the model returns a tuple of three elements:                                                                                                                                                               
        • ctc_logit: As above, the CTC predicted logits.                                                                                                                                                                                                                  
        • s2s_logit: The logits generated by the sequence-to-sequence (s2s) decoder, which aim to predict text tokens.                                                                                                                                                    
        • s2s_attn: The attention weight matrix from the decoder that aligns the audio features with positions in the predicted text.                                                                                                                                     

    Because the model provides both the s2s logits and the associated attention weights, you can view it as a text aligner. The attention weights, in particular, offer information about which portions of the audio (after being processed through the network)         
    contributed to each token prediction, effectively aligning the phonemes to the audio segments.                                                                                                                                                                        

    So, while it comes from an ASR-like architecture, its design—especially with the attention mechanism—can be leveraged to perform alignment between audio segments and text/phoneme predictions.
    """

    def __init__(self,
                 input_dim=80,
                 hidden_dim=256,
                 n_token=35,
                 n_layers=6,
                 token_embedding_dim=256,

                 ):
        """Initialize ASRCNN model.

        Parameters:
            input_dim (int): Input dimension of mel spectrogram. Expected input shape: (B, input_dim, T).
            hidden_dim (int): Hidden dimension size for convolutional layers.
            n_token (int): Number of output tokens.
            n_layers (int): Number of convolutional layers.
            token_embedding_dim (int): Dimension of token embeddings for the s2s decoder.

        Returns:
            None

        Note:
            This model converts input audio (mel spectrogram) to feature representations for both CTC and s2s decoding.
        """
        super().__init__()

        self.n_token = n_token
        self.n_down = 1
        self.to_mfcc = MFCC()
        self.init_cnn = ConvNorm(input_dim // 2, hidden_dim, kernel_size=7, padding=3, stride=2)
        self.cnns = nn.Sequential(
            *[nn.Sequential(
                ConvBlock(hidden_dim),
                nn.GroupNorm(num_groups=1, num_channels=hidden_dim)
            ) for n in range(n_layers)])
        self.projection = ConvNorm(hidden_dim, hidden_dim // 2)
        self.ctc_linear = nn.Sequential(
            LinearNorm(hidden_dim // 2, hidden_dim),
            nn.ReLU(),
            LinearNorm(hidden_dim, n_token))
        self.asr_s2s = ASRS2S(
            embedding_dim=token_embedding_dim,
            hidden_dim=hidden_dim // 2,
            n_token=n_token)

    def forward(self, x, src_key_padding_mask=None, text_input=None):
        """Perform a forward pass.
        
        Parameters:
            x (torch.Tensor): Input mel spectrogram tensor of shape (B, input_dim, T).
            src_key_padding_mask (torch.Tensor, optional): Padding mask for the input sequence.
            text_input (torch.Tensor, optional): Tokenized text input of shape (B, T_text) for s2s decoding.
        
        Returns:
            If text_input is None:
                torch.Tensor: CTC logits of shape (B, T', n_token).
            Else:
                tuple: (ctc_logit, s2s_logit, s2s_attn) where:
                    ctc_logit (torch.Tensor): CTC logits of shape (B, T', n_token).
                    s2s_logit (torch.Tensor): S2S logits of shape (B, T_text+1, n_token).
                    s2s_attn (torch.Tensor): Attention weights of shape (B, T_text+1, T').
        """
        x = self.to_mfcc(x)
        x = self.init_cnn(x)
        x = self.cnns(x)

        x = self.projection(x)
        x = x.transpose(1, 2)
        ctc_logit = self.ctc_linear(x)
        if text_input is not None:
            _, s2s_logit, s2s_attn = self.asr_s2s(x, src_key_padding_mask, text_input)
            return ctc_logit, s2s_logit, s2s_attn
        else:
            return ctc_logit

    def get_feature(self, x):
        """Extract intermediate features from the input.

        Parameters:
            x (torch.Tensor): Input mel spectrogram tensor of shape (B, input_dim, T).

        Returns:
            torch.Tensor: Feature representation from the CNN layers.
        """
        x = self.to_mfcc(x)
        x = self.init_cnn(x)
        x = self.cnns(x)
        x = self.instance_norm(x)
        x = self.projection(x)
        return x

    def length_to_mask(self, lengths):
        """Generate a boolean mask from sequence lengths.

        Parameters:
            lengths (torch.Tensor): A tensor of shape (B,) containing the lengths of each sequence in the batch.

        Returns:
            torch.BoolTensor: A mask tensor of shape (B, T) where T is the maximum sequence length.
        """
        mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)
        mask = torch.gt(mask + 1, lengths.unsqueeze(1)).to(lengths.device)
        return mask

    def get_future_mask(self, out_length, unmask_future_steps=0):
        """Generate an upper triangular future mask for sequence data.

        Parameters:
            out_length (int): The length of the output sequence (T). The returned mask will have shape (T, T).
            unmask_future_steps (int): Number of future steps to leave unmasked.

        Returns:
            torch.BoolTensor: A mask of shape (out_length, out_length) where mask[i, j] is True if i > j + unmask_future_steps, otherwise False.
        """
        index_tensor = torch.arange(out_length).unsqueeze(0).expand(out_length, -1)
        mask = torch.gt(index_tensor, index_tensor.T + unmask_future_steps)
        return mask


class ASRS2S(nn.Module):
    def __init__(self,
                 embedding_dim=256,
                 hidden_dim=512,
                 n_location_filters=32,
                 location_kernel_size=63,
                 n_token=40):
        """Initialize ASRS2S sequence-to-sequence decoder.

        Parameters:
            embedding_dim (int): Dimension of token embeddings.
            hidden_dim (int): Hidden dimension size for the decoder.
            n_location_filters (int): Number of filters for location-based attention.
            location_kernel_size (int): Kernel size for the location-based attention convolution.
            n_token (int): Number of tokens (vocabulary size) including special tokens.
        
        Returns:
            None

        Note:
            The decoder uses an LSTM cell and an attention mechanism to generate token logits and attention alignments.
        """
        super(ASRS2S, self).__init__()
        self.embedding = nn.Embedding(n_token, embedding_dim)
        val_range = math.sqrt(6 / hidden_dim)
        self.embedding.weight.data.uniform_(-val_range, val_range)

        self.decoder_rnn_dim = hidden_dim
        self.project_to_n_symbols = nn.Linear(self.decoder_rnn_dim, n_token)
        self.attention_layer = Attention(
            self.decoder_rnn_dim,
            hidden_dim,
            hidden_dim,
            n_location_filters,
            location_kernel_size
        )
        self.decoder_rnn = nn.LSTMCell(self.decoder_rnn_dim + embedding_dim, self.decoder_rnn_dim)
        self.project_to_hidden = nn.Sequential(
            LinearNorm(self.decoder_rnn_dim * 2, hidden_dim),
            nn.Tanh())
        self.sos = 1
        self.eos = 2

    def initialize_decoder_states(self, memory, mask):
        """Initialize decoder states for the sequence-to-sequence module.

        Parameters:
            memory (torch.Tensor): Encoder outputs of shape (B, L, H), where B is batch size, L is sequence length, and H is hidden dimension.
            mask (torch.Tensor): Boolean mask of shape (B, L) indicating padded positions.
        
        Returns:
            None
        """
        B, L, H = memory.shape
        self.decoder_hidden = torch.zeros((B, self.decoder_rnn_dim)).type_as(memory)
        self.decoder_cell = torch.zeros((B, self.decoder_rnn_dim)).type_as(memory)
        self.attention_weights = torch.zeros((B, L)).type_as(memory)
        self.attention_weights_cum = torch.zeros((B, L)).type_as(memory)
        self.attention_context = torch.zeros((B, H)).type_as(memory)
        self.memory = memory
        self.processed_memory = self.attention_layer.memory_layer(memory)
        self.mask = mask
        self.unk_index = 3
        self.random_mask = 0.1

    def forward(self, memory, memory_mask, text_input):
        """Perform a forward decoding pass for the sequence-to-sequence module.

        Parameters:
            memory (torch.Tensor): Encoder outputs of shape (B, L, H), where B is the batch size,
                L is the sequence length, and H is the hidden dimension.
            memory_mask (torch.Tensor): Boolean mask of shape (B, L) indicating padded positions.
            text_input (torch.Tensor): Tokenized text input of shape (B, T) to guide decoding.

        Returns:
            tuple: (hidden_outputs, logit_outputs, alignments) where:
                hidden_outputs (torch.Tensor): Decoder hidden states of shape (B, T_out, hidden_dim).
                logit_outputs (torch.Tensor): Token logits of shape (B, T_out, n_token).
                alignments (torch.Tensor): Attention weights of shape (B, T_out, L).
        """
        self.initialize_decoder_states(memory, memory_mask)
        # text random mask
        random_mask = (torch.rand(text_input.shape) < self.random_mask).to(text_input.device)
        _text_input = text_input.clone()
        _text_input.masked_fill_(random_mask, self.unk_index)
        decoder_inputs = self.embedding(_text_input).transpose(0, 1)  # -> [T, B, channel]
        start_embedding = self.embedding(
            torch.LongTensor([self.sos] * decoder_inputs.size(1)).to(decoder_inputs.device))
        decoder_inputs = torch.cat((start_embedding.unsqueeze(0), decoder_inputs), dim=0)

        hidden_outputs, logit_outputs, alignments = [], [], []
        while len(hidden_outputs) < decoder_inputs.size(0):

            decoder_input = decoder_inputs[len(hidden_outputs)]
            hidden, logit, attention_weights = self.decode(decoder_input)
            hidden_outputs += [hidden]
            logit_outputs += [logit]
            alignments += [attention_weights]

        hidden_outputs, logit_outputs, alignments = \
            self.parse_decoder_outputs(
                hidden_outputs, logit_outputs, alignments)

        return hidden_outputs, logit_outputs, alignments

    def decode(self, decoder_input):
        """Decode a single time step using the current decoder state and input.

        Parameters:
            decoder_input (torch.Tensor): Input tensor for the current time step, shape (B, embedding_dim).

        Returns:
            tuple: (hidden, logit, attention_weights) where:
                hidden (torch.Tensor): Updated hidden state, shape (B, decoder_rnn_dim).
                logit (torch.Tensor): Token logits, shape (B, n_token).
                attention_weights (torch.Tensor): Attention weights, shape (B, L).
        """
        cell_input = torch.cat((decoder_input, self.attention_context), -1)
        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(
            cell_input,
            (self.decoder_hidden, self.decoder_cell))

        attention_weights_cat = torch.cat(
            (self.attention_weights.unsqueeze(1),
             self.attention_weights_cum.unsqueeze(1)), dim=1)

        self.attention_context, self.attention_weights = self.attention_layer(
            self.decoder_hidden,
            self.memory,
            self.processed_memory,
            attention_weights_cat,
            self.mask)

        self.attention_weights_cum += self.attention_weights

        hidden_and_context = torch.cat((self.decoder_hidden, self.attention_context), -1)
        hidden = self.project_to_hidden(hidden_and_context)

        # dropout to increasing g
        logit = self.project_to_n_symbols(F.dropout(hidden, 0.5, self.training))

        return hidden, logit, self.attention_weights

    def parse_decoder_outputs(self, hidden, logit, alignments):
        """Stack and format decoder outputs from a list of time steps.

        Parameters:
            hidden (list of torch.Tensor): List of hidden state tensors for each time step, each of shape (B, hidden_dim).
            logit (list of torch.Tensor): List of logit tensors for each time step, each of shape (B, n_token).
            alignments (list of torch.Tensor): List of attention weight tensors for each time step, each of shape (B, L).

        Returns:
            tuple: (hidden, logit, alignments) where:
                hidden (torch.Tensor): Tensor of shape (B, T_out+1, hidden_dim).
                logit (torch.Tensor): Tensor of shape (B, T_out+1, n_token).
                alignments (torch.Tensor): Tensor of shape (B, T_out+1, L).
        """
        # -> [B, T_out + 1, max_time]
        alignments = torch.stack(alignments).transpose(0, 1)
        # [T_out + 1, B, n_symbols] -> [B, T_out + 1,  n_symbols]
        logit = torch.stack(logit).transpose(0, 1).contiguous()
        hidden = torch.stack(hidden).transpose(0, 1).contiguous()

        return hidden, logit, alignments
